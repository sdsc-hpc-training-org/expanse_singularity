<h3 id="run-the-container-on-expanse">Run the Container on Expanse</h3>
<p>Now that we have a Singularity container on Expanse, letâ€™s check how to use it.</p>
<p>Navigate to your scratch directory on Expanse:</p>
<pre><code>cd /expanse/lustre/scratch/$USER/temp_project</code></pre>
<p>Next, you should request an interactive session on one of Expanse's compute, debug, or shared nodes.</p>
<pre><code>srun --pty --nodes=1 --ntasks-per-node=10 -A  -p compute -t 01:00:00  --wait 0 /bin/bash </code></pre>
<p>Once your request is approved, your command prompt should reflect the new node id.</p>
<p>Before you can run your container you will need to load the Singularity module (if you are unfamiliar with modules on Expanse, you may want to review the <a href="https://www.sdsc.edu/support/user_guides/expanse.html">Expanse User Guide</a>). The command to load Singularity on Expanse is:</p>
<pre><code>module load singularitypro</code></pre>
<p>You may issue the above command from any directory on Expanse. Let's try executing the following commands:</p>
<pre><code>CONTAINER=&#39;scipy-notebook_latest.sif&#39;
python3 --version
Python 3.6.8
singularity shell $CONTAINER
Singularity&gt; python3 --version
Python 3.10.4
Singularity&gt; exit</code></pre>
<p>At this point we can create a simple Slurm script named <code>run-singularity.slurm</code> that launches a job executing a command via our Singularity container, using <code>singularity exec</code>:</p>
<pre><code>#!/usr/bin/env bash
#SBATCH --job-name=run-singularity
#SBATCH --account=&lt;ADD YOUR PROJECT HERE&gt;
### Shared partition using 1/4 of a node
#SBATCH --partition=shared
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=32
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --time=00:10:00
#SBATCH --output=log-run-singularity.o%j.%N

declare -xr SINGULARITY_MODULE=&#39;singularitypro/3.9&#39;

module purge
module load &quot;${SINGULARITY_MODULE}&quot;
module list

CONTAINER_FILE=&#39;scipy-notebook_latest.sif&#39;

time -p singularity exec --bind /expanse,/scratch $CONTAINER_FILE \
        python3 -c &quot;import platform; print(platform.python_version())&quot;</code></pre>
<p>Singularity containers by default have access to the user home folder; notice that in the call to <code>singularity exec</code>, we also specify <code>--bind /expanse,/scratch</code> so that the Project, Lustre scratch and local scratch filesystems are accessible from the container.</p>
<p>Finally we can submit it with:</p>
<pre><code>sbatch run-singularity.slurm</code></pre>
<p>In a batch script, we can also mix commands from the host and the container. For example, we can do a bash <code>for</code> loop on the host, pass the loop variable as argument to <code>singularity exec</code>, and redirect the output to a suitably named file on the host:</p>
<pre><code>for datafile in *.hdf5
do
    singularity exec $CONTAINER_FILE python3 process_data.py $datafile &gt; ${datafile}_execution.log
done</code></pre>
